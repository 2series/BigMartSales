---
title: "Big Mart Sales Prediction"
author: "Rihad Variawa"
date: "1/13/2019"
output: html_document
---

# Load Packages and Data

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## install packages if necessary
list.of.packages <- c("data.table", "dplyr", "ggplot2", "caret", "corrplot", "xgboost", "cowplot")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)


library(data.table) # used for reading and manipulation of data
library(dplyr)      # used for data manipulation and joining
library(ggplot2)    # used for ploting 
library(caret)      # used for modeling
library(corrplot)   # used for making correlation plot
library(xgboost)    # used for building XGBoost model
library(cowplot)    # used for combining multiple plots

## set working directory
knitr::opts_knit$set(root.dir = '/cloud/project')
```

```{r}
train = fread("Train_UWu5bXk.csv") 
test = fread("Test_u94Q5KV.csv")
submission = fread("SampleSubmission_TmnO39y.csv")
```

# Get to know the Data

```{r}
dim(train); dim(test)
```

```{r}
str(train)
```

```{r}
str(test)
```

## Combine Train and Test

```{r}
test[,Item_Outlet_Sales := NA]
combi = rbind(train, test) # combining train and test datasets
dim(combi)
```

# Univariate Analysis

## Target Variable

As our target variable (Item_Outlet_Sales) is continuous. We can visualise it by plotting its histogram.

```{r}
ggplot(train) + geom_histogram(aes(train$Item_Outlet_Sales), binwidth = 100, fill = "yellow") +
  xlab("Item_Outlet_Sales")
```

Observations:

  + As you can see, it is a right skewd variable and would need some data transformation to treat its skewness.

## Independent Variables (Numeric Variables)

Let’s preview the numeric independent variables. We’ll again use the histograms for visualizations because that will help us in visualizing the distribution of the variables.

```{r}
p1 = ggplot(combi) + geom_histogram(aes(Item_Weight), binwidth = 0.5, fill = "blue")
p2 = ggplot(combi) + geom_histogram(aes(Item_Visibility), binwidth = 0.005, fill = "blue")
p3 = ggplot(combi) + geom_histogram(aes(Item_MRP), binwidth = 1, fill = "blue")
plot_grid(p1, p2, p3, nrow = 1) # plot_grid() from cowplot package
```

Observations:

  + There appears to be no clear-cut pattern in Item_Weight.
  + Item_Visibility is right-skewed and should be transformed to curb its skewness.
  + We can clearly see 4 different distributions for Item_MRP. **It's an interesting insight!**

## Independent Variables (Categorical Variables)

Now we’ll try to explore and gain some insights from the categorical variables. A categorical variable or feature can have only a finite set of values. Let’s first plot Item_Fat_Content.

```{r}
ggplot(combi %>% group_by(Item_Fat_Content) %>% summarise(Count = n())) + 
  geom_bar(aes(Item_Fat_Content, Count), stat = "identity", fill = "coral1")
```

Observations:

  + In the figure above, ‘LF’, ‘low fat’, and ‘Low Fat’ are the same category and can be combined into one. Similarly, ‘reg’ and ‘Regular’ can be combined into one. After making these corrections we’ll plot the same figure again.

```{r}
combi$Item_Fat_Content[combi$Item_Fat_Content == "LF"] = "Low Fat"
combi$Item_Fat_Content[combi$Item_Fat_Content == "low fat"] = "Low Fat"
combi$Item_Fat_Content[combi$Item_Fat_Content == "reg"] = "Regular"
ggplot(combi %>% group_by(Item_Fat_Content) %>% summarise(Count = n())) + 
  geom_bar(aes(Item_Fat_Content, Count), stat = "identity", fill = "coral1")
```

Now let’s check the other categorical variables.

```{r}
# plot for Item_Type
p4 = ggplot(combi %>% group_by(Item_Type) %>% summarise(Count = n())) + 
  geom_bar(aes(Item_Type, Count), stat = "identity", fill = "coral1") +
  xlab("") +
  geom_label(aes(Item_Type, Count, label = Count), vjust = 0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  ggtitle("Item_Type")
```

```{r}
# plot for Outlet_Identifier
p5 = ggplot(combi %>% group_by(Outlet_Identifier) %>% summarise(Count = n())) + 
  geom_bar(aes(Outlet_Identifier, Count), stat = "identity", fill = "coral1") +
  geom_label(aes(Outlet_Identifier, Count, label = Count), vjust = 0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# plot for Outlet_Size
p6 = ggplot(combi %>% group_by(Outlet_Size) %>% summarise(Count = n())) + 
  geom_bar(aes(Outlet_Size, Count), stat = "identity", fill = "coral1") +
  geom_label(aes(Outlet_Size, Count, label = Count), vjust = 0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
second_row = plot_grid(p5, p6, nrow = 1)
plot_grid(p4, second_row, ncol = 1)
```

Observations:

  + In Outlet_Size’s plot, for 4016 observations, Outlet_Size is blank or missing. We'll check for this in the **Bivariate Analysis** to substitute the missing values in the Outlet_Size.

We’ll also check the remaining categorical variables.

```{r}
# plot for Outlet_Establishment_Year
p7 = ggplot(combi %>% group_by(Outlet_Establishment_Year) %>% summarise(Count = n())) + 
  geom_bar(aes(factor(Outlet_Establishment_Year), Count), stat = "identity", fill = "coral1") +
  geom_label(aes(factor(Outlet_Establishment_Year), Count, label = Count), vjust = 0.5) +
  xlab("Outlet_Establishment_Year") +
  theme(axis.text.x = element_text(size = 8.5))
```

```{r}
# plot for Outlet_Type
p8 = ggplot(combi %>% group_by(Outlet_Type) %>% summarise(Count = n())) + 
  geom_bar(aes(Outlet_Type, Count), stat = "identity", fill = "coral1") +
  geom_label(aes(factor(Outlet_Type), Count, label = Count), vjust = 0.5) +
  theme(axis.text.x = element_text(size = 8.5))
```

```{r}
# ploting both plots together
plot_grid(p7, p8, ncol = 2)
```

Observations:

  + Lesser number of observations in the data for the outlets established in the year 1998 as compared to the other years.
  + Supermarket Type 1 seems to be the most popular category of Outlet_Type.

# Bivariate Analysis

After looking at every feature individually, let’s now do some **bivariate analysis.** Here, we’ll explore the independent variables with respect to the target variable. The objective is to discover hidden relationships between the independent variable and the target variable and use those findings in missing data imputation and feature engineering.

We'll make use of **scatter plots** for the continuous or numeric variables and **violin plots** for the categorical variables.

```{r}
train = combi[1:nrow(train)] # extracting train data from the combined data
```

## Target Variable vs Independent Numerical Variables

Let’s explore the numerical variables first.

```{r}
# Item_Weight vs Item_Outlet_Sales
p9 = ggplot(train) + 
     geom_point(aes(Item_Weight, Item_Outlet_Sales), colour = "violet", alpha = 0.3) +
     theme(axis.title = element_text(size = 8.5))
```

```{r}
# Item_Visibility vs Item_Outlet_Sales
p10 = ggplot(train) + 
      geom_point(aes(Item_Visibility, Item_Outlet_Sales), colour = "violet", alpha = 0.3) +
      theme(axis.title = element_text(size = 8.5))
```

```{r}
# Item_MRP vs Item_Outlet_Sales
p11 = ggplot(train) + 
      geom_point(aes(Item_MRP, Item_Outlet_Sales), colour = "violet", alpha = 0.3) +
      theme(axis.title = element_text(size = 8.5))
```

```{r}
second_row_2 = plot_grid(p10, p11, ncol = 2)
plot_grid(p9, second_row_2, nrow = 2)
```

Observations:

  + Item_Outlet_Sales is spread well across the entire range of the Item_Weight without any obvious pattern.
  + In Item_Visibility vs Item_Outlet_Sales, there is a string of points at Item_Visibility = 0.0 which seems strange as item visibility cannot be completely zero. *We will take note of this issue and deal with it in the later stages.*
  + In the third plot of Item_MRP vs Item_Outlet_Sales, we can clearly see 4 segments of prices that can be used in feature engineering to create a new variable.

## Target Variable vs Independent Categorical Variables

Now we’ll visualise the categorical variables with respect to Item_Outlet_Sales. We will try to check the distribution of the target variable across all the categories of each of the categorical variable.

We could have used boxplots here, but instead we’ll use the violin plots as they show the full distribution of the data. The width of a violin plot at a particular level indicates the concentration or density of data at that level. The height of a violin tells us about the range of the target variable values.

```{r}
# Item_Type vs Item_Outlet_Sales
p12 = ggplot(train) + 
      geom_violin(aes(Item_Type, Item_Outlet_Sales), fill = "magenta") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            axis.text = element_text(size = 6),
            axis.title = element_text(size = 8.5))
```

```{r}
# Item_Fat_Content vs Item_Outlet_Sales
p13 = ggplot(train) + 
      geom_violin(aes(Item_Fat_Content, Item_Outlet_Sales), fill = "magenta") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            axis.text = element_text(size = 8),
            axis.title = element_text(size = 8.5))
```

```{r}
# Outlet_Identifier vs Item_Outlet_Sales
p14 = ggplot(train) + 
      geom_violin(aes(Outlet_Identifier, Item_Outlet_Sales), fill = "magenta") +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            axis.text = element_text(size = 8),
            axis.title = element_text(size = 8.5))
```

```{r}
second_row_3 = plot_grid(p13, p14, ncol = 2)
plot_grid(p12, second_row_3, ncol = 1)
```

Observations:

  + Distribution of Item_Outlet_Sales across the categories of Item_Type is not very distinct and same is the case with Item_Fat_Content.
  + The distribution for OUT010 and OUT019 categories of Outlet_Identifier are quite similar and very much different from the rest of the categories of Outlet_Identifier.

In **Univariate Analysis,** we come to know about the empty values in Outlet_Size variable. Let’s check the distribution of the target variable across Outlet_Size.

```{r}
ggplot(train) + geom_violin(aes(Outlet_Size, Item_Outlet_Sales), fill = "magenta")
```

Observations:

  + The distribution of ‘Small’ Outlet_Size is almost identical to the distribution of the blank category (first vioin) of Outlet_Size. So, we can substitute the blanks in Outlet_Size with ‘Small’.
  + *Please note that this is not the only way to impute missing values, but for the time being we will go ahead and impute the missing values with ‘Small’.*

Let’s examine the remaining variables.

```{r}
p15 = ggplot(train) + geom_violin(aes(Outlet_Location_Type, Item_Outlet_Sales), fill = "magenta")
p16 = ggplot(train) + geom_violin(aes(Outlet_Type, Item_Outlet_Sales), fill = "magenta")
plot_grid(p15, p16, ncol = 1)
```

Observations:

  + Tier 1 and Tier 3 locations of Outlet_Location_Type look similar.
  + In the Outlet_Type plot, Grocery Store has most of its data points around the lower sales values as compared to the other categories.

# Missing Value Treatment

There are different methods to treat missing values based on the problem and the data. Some of the common techniques are as follows:

1. **Deletion of rows:** In train dataset, observations having missing values in any variable are deleted. The downside of this method is the loss of information and drop in prediction power of model.

2. **Mean/Median/Mode Imputation:** In case of continuous variable, missing values can be replaced with mean or median of all known values of that variable. For categorical variables, we can use mode of the given values to replace the missing values.

3. **Building Prediction Model:** We can even make a predictive model to impute missing data in a variable. Here we will treat the variable having missing data as the target variable and the other variables as predictors. We will divide our data into 2 datasets—one without any missing value for that variable and the other with missing values for that variable. The former set would be used as training set to build the predictive model and it would then be applied to the latter set to predict the missing values.

You can try the following code to quickly find missing values in a variable.

```{r}
sum(is.na(combi$Item_Weight))
```

## Imputing Missing Values

As seen above, we have missing values in Item_Weight and Item_Outlet_Sales. Missing data in Item_Outlet_Sales can be ignored since they belong to the test dataset. We’ll now impute Item_Weight with mean weight based on the Item_Identifier variable.

```{r}
missing_index = which(is.na(combi$Item_Weight))
for(i in missing_index){
  
  item = combi$Item_Identifier[i]
  combi$Item_Weight[i] = mean(combi$Item_Weight[combi$Item_Identifier == item], na.rm = T)
}
```

Now let’s see if there is still any missing data in Item_Weight?

```{r}
sum(is.na(combi$Item_Weight))
```

Observations:
  
  + Zero missing values! GREAT. It means we have successfully imputed the missing data in the feature.

## Replacing 0’s in Item_Visibility variable

Similarly, zeroes in Item_Visibility variable can be replaced with Item_Identifier wise mean values of Item_Visibility. It can be visualized in the plot below.

```{r}
ggplot(combi) + geom_histogram(aes(Item_Visibility), bins = 100, fill = "yellow")
```

Let’s replace the zeroes.

```{r}
zero_index = which(combi$Item_Visibility == 0)
for(i in zero_index){
  
  item = combi$Item_Identifier[i]
  combi$Item_Visibility[i] = mean(combi$Item_Visibility[combi$Item_Identifier == item], na.rm = T)
}
```

After the replacement of zeroes, We’ll plot the histogram of Item_Visibility again. In the histogram, we can see that the issue of zero item visibility has been resolved.

```{r}
ggplot(combi) + geom_histogram(aes(Item_Visibility), bins = 100, fill = "yellow")
```

# Feature Engineering

In this section we will create the following new features:

  + **Item_Type_new:** Broader categories for the variable Item_Type.
  + **Item_category:** Categorical variable derived from Item_Identifier.
  + **Outlet_Years:** Years of operation for outlets.
  + **price_per_unit_wt:** Item_MRP/Item_Weight
  + **Item_MRP_clusters:** Binned feature for Item_MRP.

We can have a look at the Item_Type variable and classify the categories into **perishable** and **non_perishable** as per our understanding and make it into a new feature.

```{r}
perishable = c("Breads", "Breakfast", "Dairy", "Fruits and Vegetables", "Meat", "Seafood")
```

```{r}
non_perishable = c("Baking Goods", "Canned", "Frozen Foods", "Hard Drinks", "Health and Hygiene", "Household", "Soft Drinks")
```

```{r}
# create a new feature 'Item_Type_new'
combi[,Item_Type_new := ifelse(Item_Type %in% perishable, "perishable", ifelse(Item_Type %in% non_perishable, "non_perishable", "not_sure"))]
```

Let’s compare Item_Type with the first 2 characters of Item_Identifier, i.e., ‘DR’, ‘FD’, and ‘NC’. These identifiers most probably stand for **drinks,** **food,** and **non-consumable.**

```{r}
table(combi$Item_Type, substr(combi$Item_Identifier, 1, 2))
```

Observations:

  + Based on the above table we can create a new feature. Let’s call it Item_category.

```{r}
combi[,Item_category := substr(combi$Item_Identifier, 1, 2)]
```

We'll also change the values of *Item_Fat_Content* wherever Item_category is ‘NC’ because non-consumable items cannot have any fat content. We'll also create a couple of more features — **Outlet_Years** (years of operation) and **price_per_unit_wt** (price per unit weight).

```{r}
combi$Item_Fat_Content[combi$Item_category == "NC"] = "Non-Edible"
# years of operation for outlets
combi[,Outlet_Years := 2013 - Outlet_Establishment_Year]
combi$Outlet_Establishment_Year = as.factor(combi$Outlet_Establishment_Year)
# Price per unit weight
combi[,price_per_unit_wt := Item_MRP/Item_Weight]
```

Earlier in the Item_MRP vs Item_Outlet_Sales plot, we saw Item_MRP was spread across in 4 chunks. Now let’s assign a label to each of these chunks and use this label as a new variable.

```{r}
# creating new independent variable - Item_MRP_clusters
combi[,Item_MRP_clusters := ifelse(Item_MRP < 69, "1st", 
                                   ifelse(Item_MRP >= 69 & Item_MRP < 136, "2nd",
                                          ifelse(Item_MRP >= 136 & Item_MRP < 203, "3rd", "4th")))]
```

# Encoding Categorical Variables

## Why Encoding Categorical Variables is essential?

Most of the ML algorithms produce better result with numerical variables only. So, it is essential to treat the categorical variables present in the data. One thing that can be done is to completely remove the categorical variables, but that would lead to enormous loss of information. Fortunately we have smarter techniques to deal with the categorical variables.

We'll convert our categorical variables into numerical ones. We'll use 2 techniques — *Label Encoding* and *One Hot Encoding:*

1. **Label Encoding** simply means converting each category in a variable to a number. It is more suitable for ordinal variables — categorical variables with some order.

2. **One Hot Encoding,** each category of a categorical variable is converted into a new binary column (1/0).

## Label Encoding for the Categorical Variables

We'll label encode Outlet_Size and Outlet_Location_Type as these are ordinal variables.

```{r}
combi[,Outlet_Size_num := ifelse(Outlet_Size == "Small", 0,
                                 ifelse(Outlet_Size == "Medium", 1, 2))]
combi[,Outlet_Location_Type_num := ifelse(Outlet_Location_Type == "Tier 3", 0,
                                          ifelse(Outlet_Location_Type == "Tier 2", 1, 2))]
# removing categorical variables after label encoding
combi[, c("Outlet_Size", "Outlet_Location_Type") := NULL]
```

## One Hot Encoding for the Categorical Variable

```{r}
ohe = dummyVars("~.", data = combi[,-c("Item_Identifier", "Outlet_Establishment_Year", "Item_Type")], fullRank = T)
ohe_df = data.table(predict(ohe, combi[,-c("Item_Identifier", "Outlet_Establishment_Year", "Item_Type")]))
combi = cbind(combi[,"Item_Identifier"], ohe_df)
```

# PreProcessing Data

## What is Data Preprocessing?

In simple words, pre-processing refers to the transformations applied to your data before feeding it to the algorithm. It invloves further cleaning of data, data transformation, data scaling and many more things.

For our data, we'll deal with the skewness and scale the numerical variables.

## Removing Skewness

Skewness in variables is undesirable for predictive modeling. Some ML methods assume normally distributed data and a skewed variable can be transformed by taking its log, square root, or cube root so as to make its distribution as close to normal distribution as possible. In our data, variables Item_Visibility and price_per_unit_wt are highly skewed. So, we'll treat their skewness with the help of log transformation.

```{r}
combi[,Item_Visibility := log(Item_Visibility + 1)] # log + 1 to avoid division by zero
combi[,price_per_unit_wt := log(price_per_unit_wt + 1)]
```

## Scaling Numeric Predictors

Let’s scale and center the numeric variables to make them have a mean of zero, standard deviation of one and scale of 0 to 1. Scaling and centering is required for linear regression models.

```{r}
num_vars = which(sapply(combi, is.numeric)) # index of numeric features
num_vars_names = names(num_vars)
combi_numeric = combi[,setdiff(num_vars_names, "Item_Outlet_Sales"), with = F]
prep_num = preProcess(combi_numeric, method=c("center", "scale"))
combi_numeric_norm = predict(prep_num, combi_numeric)
```

```{r}
combi[,setdiff(num_vars_names, "Item_Outlet_Sales") := NULL] # removing numeric independent variables
combi = cbind(combi, combi_numeric_norm)
```

Split the combined data back to train and test set.

```{r}
train = combi[1:nrow(train)]
test = combi[(nrow(train) + 1):nrow(combi)]
test[,Item_Outlet_Sales := NULL] # removing Item_Outlet_Sales as it contains only NA for test dataset
```

## Correlated Variables

Let’s examine the correlated features of train dataset. Correlation varies from -1 to 1.

  + negative correlation: < 0 and >= -1
  + positive correlation: > 0 and <= 1
  + no correlation: 0

It is not desirable to have correlated features if we are using linear regressions.

```{r}
cor_train = cor(train[,-c("Item_Identifier")])
corrplot(cor_train, method = "pie", type = "lower", tl.cex = 0.9)
```

Observations:
  
  + The correlation plot above shows correlation between all the possible pairs of variables in out data. The correlation between any two variables is represented by a pie. A blueish pie indicates positive correlation and reddish pie indicates negative correlation. The magnitude of the correlation is denoted by the area covered by the pie.

  + Variables price_per_unit_wt and Item_Weight are highly correlated as the former one was created from the latter. Similarly price_per_unit_wt and Item_MRP are highly correlated for the same reason.

# Model Building

At the competition’s page, it has been mentioned that our submission data would be evaluated based on the RMSE score. Hence, we will use RMSE as our evaluation metric.

* **Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors.

# Linear Regression

Linear regression is the simplest and most widely used statistical technique for predictive modeling. 

We will use 5-fold cross validation in all the models we are going to build. Basically cross vaidation gives an idea as to how well a model generalizes to unseen data.

## Build Model

```{r}
linear_reg_mod = lm(Item_Outlet_Sales ~ ., data = train[,-c("Item_Identifier")])
```

## Make Predictions on Test Data

```{r}
# preparing dataframe for submission and writing it in a csv file
submission$Item_Outlet_Sales = predict(linear_reg_mod, test[,-c("Item_Identifier")])
write.csv(submission, "Linear_Reg_submit.csv", row.names = F)
```

# Regularized Linear Regression

**Regularised regression models** can handle the correlated independent variables well and helps in overcoming overfitting. **Ridge** penalty shrinks the coefficients of correlated predictors towards each other, while the **Lasso** tends to pick one of a pair of correlated features and discard the other. The tuning parameter **lambda** controls the strength of the penalty.

## Lasso Regression

```{r}
set.seed(1235)
my_control = trainControl(method="cv", number=5)
Grid = expand.grid(alpha = 1, lambda = seq(0.001,0.1,by = 0.0002))

lasso_linear_reg_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales,
                       method='glmnet', trControl= my_control, tuneGrid = Grid)
```

## Ridge Regression

```{r}
set.seed(1236)
my_control = trainControl(method="cv", number=5)
Grid = expand.grid(alpha = 0, lambda = seq(0.001,0.1,by = 0.0002))

ridge_linear_reg_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], y = train$Item_Outlet_Sales,
                       method='glmnet', trControl= my_control, tuneGrid = Grid)
```

# Random Forest

**RandomForest** is a tree based bootstrapping algorithm wherein a certain number of weak learners (decision trees) are combined to make a powerful prediction model. For every individual learner, a random sample of rows and a few randomly chosen variables are used to build a decision tree model. Final prediction can be a function of all the predictions made by the individual learners. In case of a regression problem, the final prediction can be mean of all the predictions. 

We'll now build a RandomForest model with 400 trees. The other tuning parameters used here are mtry — no. of predictor variables randomly sampled at each split, and min.node.size — minimum size of terminal nodes (setting this number large causes smaller trees and reduces overfitting).

```{r}
set.seed(1237)
my_control = trainControl(method="cv", number=5) # 5-fold CV
tgrid = expand.grid(
  .mtry = c(3:10),
  .splitrule = "variance",
  .min.node.size = c(10,15,20)
)
rf_mod = train(x = train[, -c("Item_Identifier", "Item_Outlet_Sales")], 
               y = train$Item_Outlet_Sales,
               method='ranger', 
               trControl= my_control, 
               tuneGrid = tgrid,
               num.trees = 400,
               importance = "permutation")
```

## Best Model Parameters

```{r}
plot(rf_mod)
```

Observations:
  
  + As per the plot shown above, the best score is achieved at mtry = 5 and min.node.size = 20.

## Variable Importance

Let’s plot feature importance based on the RandomForest model

```{r}
plot(varImp(rf_mod))
```

Observations:

  + As expected Item_MRP is the most important variable in predicting the target variable. New features created by us, like price_per_unit_wt, Outlet_Years, Item_MRP_Clusters, are also among the top most important variables. This is why feature engineering plays such a crucial role in predictive modeling.

# XGBoost

XGBoost is a fast and efficient algorithm and has been used to by the winners of many data science competitions. XGBoost works only with numeric variables and we have already replaced the categorical variables with numeric variables. There are many tuning parameters in XGBoost which can be broadly classified into General Parameters, Booster Parameters and Task Parameters.

*General parameters refer to which booster we are using to do boosting. The commonly used are tree or linear model.
*Booster parameters depend on which booster you have chosen.
*Learning Task parameters that decide on the learning scenario, for example, regression tasks may use different parameters with ranking tasks.

Let’s preview the parameters that we are going to use in our model.

1. **eta:** It is also known as the learning rate or the shrinkage factor. It actually shrinks the feature weights to make the boosting process more conservative. The range is 0 to 1. Low eta value means the model is more robust to overfitting.
2. **gamma:** The range is 0 to ∞. Larger the gamma more conservative the algorithm is.
3. **max_depth:** We can specify maximum depth of a tree using this parameter.
4. **subsample:** It is the proportion of rows that the model will randomly select to grow trees.
5. **colsample_bytree:** It is the ratio of variables randomly chosen to build each tree in the model.

```{r}
param_list = list(
        
        objective = "reg:linear",
        eta=0.01,
        gamma = 1,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.5
        )
```

```{r}
dtrain = xgb.DMatrix(data = as.matrix(train[,-c("Item_Identifier", "Item_Outlet_Sales")]), label= train$Item_Outlet_Sales)
dtest = xgb.DMatrix(data = as.matrix(test[,-c("Item_Identifier")]))
```

## Cross Validation

We are going to use the xgb.cv() function for cross validation. This function comes with the xgboost package itself. Here we are using cross validation for finding the optimal value of nrounds.

```{r}
set.seed(112)
xgbcv = xgb.cv(params = param_list, 
               data = dtrain, 
               nrounds = 1000, 
               nfold = 5, 
               print_every_n = 10, 
               early_stopping_rounds = 30, 
               maximize = F)
```

## Model Training

As per the verbose above, we got the best validation/test score at the 430th iteration. Hence, we will use nrounds = 430 for building the XGBoost model.

```{r}
xgb_model = xgb.train(data = dtrain, params = param_list, nrounds = 430)
```

## Variable Importance

```{r}
var_imp = xgb.importance(feature_names = setdiff(names(train), c("Item_Identifier", "Item_Outlet_Sales")), 
                         model = xgb_model)
xgb.plot.importance(var_imp)
```

Observations:

  + Again the features created by us, like price_per_unit_wt, Outlet_Years, Item_MRP_Clusters, are among the top most important variables.
